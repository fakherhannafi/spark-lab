{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fa1589",
   "metadata": {},
   "source": [
    "#### This Notebook aims to test the good installation of Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c7fc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.1.3-bin-hadoop3.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing pyspark installation\n",
    "# you have to install findspark with this command: conda install -c conda-forge findspark\n",
    "# Make sure if you are behind a corporate proxy to whitelist conda-forge channel\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e03fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:2.7.1 --conf spark.dynamicAllocation.enabled=true pyspark-shell '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9636dff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22380/161988250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# configure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mSparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.hadoop.fs.s3a.endpoint\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3-eu-west-1.amazonaws.com\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: set() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# configure\n",
    "#SparkConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3-eu-west-1.amazonaws.com\")\n",
    "conf = SparkConf()\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')\n",
    "\n",
    "# Hadoop Conf\n",
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set('fs.s3a.access.key', 'AKIARPSVTGQX3KE4QSEL')\n",
    "hadoopConf.set(\"fs.s3a.secret.key\",\n",
    "               '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')\n",
    "#hadoopConf.set('spark.hadoop.fs.s3a.endpoint', 's3-eu-west-1.amazonaws.com')\n",
    "hadoopConf.set('com.amazonaws.services.s3a.enableV4', 'true')\n",
    "hadoopConf.set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "hadoopConf.set('fs.s3a.multipart.size', \"104857600\")\n",
    "hadoopConf.set(\"fs.s3a.threads.max\", \"4\")\n",
    "hadoopConf.set(\"fs.s3a.threads.core\", \"4\")\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#df = spark.read.csv(\"s3a://mylab-2022/listings.csv\")\n",
    "\n",
    "\n",
    "df = spark.read.format('csv').options(header='true', inferSchema='true').load('s3a://mylab-2022/listings.csv')\n",
    "\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d18bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Test Code\n",
    "num_val = sc.parallelize([1,2,3,5])\n",
    "num_val.map(lambda x:x*x*x).collect()\n",
    "# Make sure py4j is installed in the kernel used by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde19d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .option(\"quote\", '\"')\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"s3a://mylab-2022/listings.csv\")\\\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4915158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
